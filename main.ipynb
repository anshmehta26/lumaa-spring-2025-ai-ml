{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsR6qpfHOn5pbSo7DBSS7+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshmehta26/lumaa-spring-2025-ai-ml/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HXaVOLzS9OS",
        "outputId": "3df97050-ed15-43b7-9a83-42c55e1904b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Connecting to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accessing stored csv file and converting it into a Pandas DataFrame before displaying how many movies it contains\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/500_netflix_movies.csv')\n",
        "\n",
        "print(\"The database has \" + str(len(df))+ \" movies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PI593WUTp17",
        "outputId": "247283a6-b9d8-4eac-824c-c0cf81318c44"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The database has 500 movies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import string\n",
        "\n",
        "# Basic Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Lowercase the text, remove punctuation, and split into tokens.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tokens.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = text.split()\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "iSmox54KU4mG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic TF-IDF Vectorizer Implementation\n",
        "\n",
        "class BasicTfidfVectorizer:\n",
        "    def __init__(self):\n",
        "        self.vocabulary_ = {}  # Maps token to index\n",
        "        self.idf_ = {}         # Maps token to its IDF value\n",
        "        self.N = 0             # Number of documents\n",
        "\n",
        "    def fit(self, documents):\n",
        "        \"\"\"\n",
        "        Build vocabulary and compute IDF values from the documents.\n",
        "\n",
        "        Parameters:\n",
        "            documents (list of str): The corpus of documents.\n",
        "        \"\"\"\n",
        "        self.N = len(documents)\n",
        "        doc_freq = {}\n",
        "\n",
        "        # Count document frequency for each token (unique tokens per document)\n",
        "        for doc in documents:\n",
        "            tokens = set(preprocess_text(doc))\n",
        "            for token in tokens:\n",
        "                doc_freq[token] = doc_freq.get(token, 0) + 1\n",
        "\n",
        "        # Build vocabulary and compute IDF for each token\n",
        "        self.vocabulary_ = {}\n",
        "        self.idf_ = {}\n",
        "        for i, token in enumerate(doc_freq.keys()):\n",
        "            self.vocabulary_[token] = i\n",
        "            # Using smoothing: idf = log((N + 1) / (df + 1)) + 1\n",
        "            self.idf_[token] = math.log((self.N + 1) / (doc_freq[token] + 1)) + 1\n",
        "\n",
        "    def transform(self, documents):\n",
        "        \"\"\"\n",
        "        Transform documents into TF-IDF vectors.\n",
        "\n",
        "        Parameters:\n",
        "            documents (list of str): Documents to transform.\n",
        "\n",
        "        Returns:\n",
        "            list of list: Each document represented as a list of TF-IDF values.\n",
        "        \"\"\"\n",
        "        vectors = []\n",
        "        for doc in documents:\n",
        "            tokens = preprocess_text(doc)\n",
        "            tf = {}\n",
        "            # Compute term frequency for each token in the document\n",
        "            for token in tokens:\n",
        "                if token in self.vocabulary_:\n",
        "                    tf[token] = tf.get(token, 0) + 1\n",
        "            # Normalize term frequencies by the maximum frequency in the document\n",
        "            max_freq = max(tf.values()) if tf else 1\n",
        "            vec = [0.0] * len(self.vocabulary_)\n",
        "            for token, count in tf.items():\n",
        "                tf_norm = count / max_freq\n",
        "                index = self.vocabulary_[token]\n",
        "                vec[index] = tf_norm * self.idf_[token]\n",
        "            vectors.append(vec)\n",
        "        return vectors\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        \"\"\"\n",
        "        Fit to the documents and transform them into TF-IDF vectors.\n",
        "\n",
        "        Parameters:\n",
        "            documents (list of str): The corpus of documents.\n",
        "\n",
        "        Returns:\n",
        "            list of list: TF-IDF vectors for each document.\n",
        "        \"\"\"\n",
        "        self.fit(documents)\n",
        "        return self.transform(documents)\n"
      ],
      "metadata": {
        "id": "5BzjBowKXBoC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Cosine similarity function\n",
        "def cosine_similarity_vec(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two vectors using numpy.\n",
        "\n",
        "    Parameters:\n",
        "        vec1 (list or np.array): First vector.\n",
        "        vec2 (list or np.array): Second vector.\n",
        "\n",
        "    Returns:\n",
        "        float: Cosine similarity score.\n",
        "    \"\"\"\n",
        "    # Convert the vectors to numpy arrays if they're not already\n",
        "    vec1 = np.array(vec1)\n",
        "    vec2 = np.array(vec2)\n",
        "\n",
        "    # Compute the dot product using numpy\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "\n",
        "    # Compute the L2 norms (Euclidean norms) of the vectors\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return dot_product / (norm1 * norm2)\n"
      ],
      "metadata": {
        "id": "hWgBvZgHaK9f"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendation Function\n",
        "\n",
        "def recommend_items(query, vectorizer, tfidf_matrix, df, top_n=5):\n",
        "    \"\"\"\n",
        "    Given a user query, compute cosine similarity between the query and each movie description.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): User's text describing their preferences.\n",
        "        vectorizer (BasicTfidfVectorizer): A fitted basic TF-IDF vectorizer.\n",
        "        tfidf_matrix (list of list): TF-IDF vectors for all movie descriptions.\n",
        "        df (pd.DataFrame): DataFrame containing the movies with a 'title' column.\n",
        "        top_n (int): Number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples (movie title, similarity score).\n",
        "    \"\"\"\n",
        "    # Transform the query using the same vectorizer\n",
        "    query_vec = vectorizer.transform([query])[0]\n",
        "\n",
        "    # Compute cosine similarity between the query and each movie description\n",
        "    similarities = []\n",
        "    for idx, doc_vec in enumerate(tfidf_matrix):\n",
        "        score = cosine_similarity_vec(query_vec, doc_vec)\n",
        "        similarities.append((idx, score))\n",
        "\n",
        "    # Sort the movies by similarity score in descending order\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Retrieve the top_n recommendations\n",
        "    recommendations = [(df.iloc[idx]['title'], score) for idx, score in similarities[:top_n]]\n",
        "    return recommendations\n"
      ],
      "metadata": {
        "id": "wwr_OGdpbAB9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example implementation\n",
        "\n",
        "#Accessing stored csv file and converting it into a Pandas DataFrame before displaying how many movies it contains\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/500_netflix_movies.csv')\n",
        "print(f\"Number of movies in dataset: {len(df)}\")\n",
        "\n",
        "# Build the TF-IDF vectorizer using the 'description' column\n",
        "# Fill missing descriptions with an empty string\n",
        "descriptions = df['description'].fillna(\"\").tolist()\n",
        "vectorizer = BasicTfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
        "\n",
        "# Example user query\n",
        "query = \"I love thrilling action movies set in space, with a comedic twist.\"\n",
        "\n",
        "# Get recommendations\n",
        "recommendations = recommend_items(query, vectorizer, tfidf_matrix, df, top_n=5)\n",
        "\n",
        "# Print the top recommendations\n",
        "print(\"Top Recommendations:\")\n",
        "for title, score in recommendations:\n",
        "    print(f\"{title} (Similarity Score: {score:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ohw9FQ4NbYs7",
        "outputId": "278ad98f-b9e7-43df-ba17-0d1c049bc9c8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of movies in dataset: 500\n",
            "Top Recommendations:\n",
            "A StoryBots Space Adventure (Similarity Score: 0.23)\n",
            "2 Hearts (Similarity Score: 0.22)\n",
            "Chhota Bheem: Bheem vs Aliens (Similarity Score: 0.13)\n",
            "Boyka: Undisputed (Similarity Score: 0.13)\n",
            "SAS: Rise of the Black Swan (Similarity Score: 0.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salary Expectation: **40 dollars/hour** atleast which translates to **6900(approx)/month**. Can be negotiated if appropriate housing/transportation is provided. I am a student at Cornell University so relocating from Ithaca will be a challenge as long as there is appropriate compensation to help me move and start my job the hourly wage is negotiable."
      ],
      "metadata": {
        "id": "MCQK1oTJeoS_"
      }
    }
  ]
}